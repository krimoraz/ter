\documentclass[11pt,a4paper,french]{article}

\input{../../commons.tex.inc}

\title{Lois de probabilité à densité}
\author{\bsc{Jumel}}
\date{avril 2018}

\begin{document}

\maketitle

\bigskip

\section{La loi uniforme, un exemple pour comprendre}

Le modèle utilisé dans cet exemple est celui du tirage aléatoire d'un
nombre réel entre deux réels $a$ et $b$. Il est très similaire au tirage
d'un nombre au hasard entre 0 et 1.

\begin{exercice}
  Justifier l'affirmation précédente.
\end{exercice}

\begin{tikzpicture}
  \draw (0,0) -- (5,0) ;
  \draw (1,0) node { $\large [$ };
  \draw (1,-.5) node { $a$ };
  \draw (4,0) node { $\large ]$ };
  \draw (4,-.5) node { $b$ };

  \draw[red] (1.7,0) node { $\large [$ } --(3.3,0) node { $\large ]$ };
  \draw[red] (2.5,-0.5) node { $J$ };
\end{tikzpicture}

On choisit au hasard $x \in \interff{a;b}$ et on cherche la probabilité
que $x$ appartiennent à $J$, notée $\p(x\in J)$.

Supposons que $J = \interff{\frac{p}q ; \frac{p'}q}$, avec $p$, $p'$ et
$q$ convenablement choisis. La probabilité d'appartenir à un tel
intervalle est $\frac{p' - p}{q(b-a)}$.

Divisons désormais l'intervalle $\interff{a;b}$ en $n$ intervalle de
longueur $\frac{b-a}n$. Sur tous ces intervalles $J_i = \interff{a_i +
i\frac{b-a}n ; a_i + (i+1)\frac{b-a}n}$, la probabilité que $x$
appartiennent à un de ces intervalles est constante et vaut $p$ et
la probabilité qu'il appartiennent à la réunion des intervalles vaut 1.

Un intervalle arbitraire $K$ peut-être vu comme réunion d'intervalles
$J_i$ pour un $n$ donné : $K \bigcup_{i = l}^m J_i$. Avec notre
construction, on peut aussi bien considérer que nos intervalles sont
ouvert ou fermés, on verra pourquoi par la suite.

On a donc, en généralisant une formule connue : $\p(x\in K) =
\sum_{i=l}^m \p(x\in J_i) = (m -l)p$.

On obtient donc que $p = \frac1{b-a}$.

\begin{exercice}
  \begin{enumerate}
    \item Rappeler la formule qui a permis cette généralisation.
    \item Quel argument permet d'obtenir le dernier résultat ?
    \item Proposer une interprétation «robuste» du fait que $\sum_{i =
      a}^b \p(x\in J_i) = 1$ qui permettra une généralisation.
  \end{enumerate}
\end{exercice}

\section{Lois à densité}

\subsection{Généralités}

On va désormais généraliser les résultats précédents. On va admettre
quelques résultats que nous n'exploiterons que dans le cadre de ce cours
et qui nous ameneraient à des développements théoriques très longs.

En particulier, on admet que, sous certaines conditions, on peut écrire
$\int_a^{+\infty}f(x)\diff x$ et affirmer qu'il s'agit d'un nombre réel.
C'est bien souvent un résultat obtenu à partir de
$\lim_{b\to+\infty}\int_a^b f(x) \diff x$.

Ceci étant posé, on peut écrire la première définition formelle du
chapitre.

\begin{definition}
  On appelle \emph{densité de probabilité} une fonction $f$ définie et
  continue sur $I$ un intervalle (non nécessairement borné) de $\R$
  telle que son intégrale sur $I$ $\int_{x\in I}f(x) \diff x = 1$.
\end{definition}

On considère désormais une variable aléatoire $X$ à valeurs dans $I$. On
notera de façon abusive $X \in J$pour indiquer que l'image $X(\omega)$
d'un évenement appartient à l'intervalle (non nécessairement borné) $J$.

\begin{definition}
  On dit que la variable aléatoire \emph{suit la loi de densité $f$}
  lorsque $\p(X\in J) = \int_{J} f(x)\diff{x}$.
\end{definition}

\begin{remarque}
  \begin{exercice}~\\
    Démontrer qu'ainsi, pour $J$ et $K$ deux intervalles de $I$, on a :
    \begin{itemize}
      \item $\p(X\in \overline{J}) = 1 - \p(X\in J)$
      \item $\p(X\in J \cup K) + p(X\in J \cap K) = \p(X\in J) + \p(X\in K)$
    \end{itemize}
    Justifier que, pour une loi à densité, $\p(X \in \{a\}) = \p(X = a)
    = 0$.
  \end{exercice}
\end{remarque}

On notera souvent $\p(X\in \{x\in \R | a\leqslant x \leqslant b\})$ par
$\p(a \leqslant X \leqslant b)$ par abus «inverse» de notation.

\begin{propriete}
  Pour toute variable aléatoire $X$ qui suit une loi à densité, on a
  \begin{itemize}
    \item $\p(X = a) = 0$
    \item $\p(X \leqslant a ) = \p(X < a)$
  \end{itemize}
\end{propriete}

On définit également, par analogie avec les lois dites discrètes,
l'espérance d'une variable aléatoire $X$ dans le cas où celle-ci suit
une loi de densité de probabilité $f$.

\begin{definition}
  Sous réserve d'existence de l'intégrale, \emph{l'espérance} de la
  variable aléatoire $X$ est définie par $E(x) = \int_I xf(x) \diff x$.
\end{definition}

\subsection{La loi uniforme}

Revenons à notre exemple premier de loi uniforme sur le segment
$\interff{a;b}$.

\begin{definition}
  La loi de probabilité uniforme a pour densité de probabilité la
  fonction continue défine sur $\interff{a;b}$ par $f(x) = \frac1{b-a}$
\end{definition}

On vérifie immédiatement que cette fonction définit bien une densité de
probabilité.

\begin{exercice}
  Calculer $\int_a^b f(x) \diff x$.
\end{exercice}

On parle de loi de probabilité uniforme car si $J$ et $K$ sont deux
intervalles de même longueur, $\p(X\in J) = \p(X \in K)$

\begin{exercice}
  Montrer que si $X$ suit une loi uniforme, alors $E(X) = \frac{a+b}2$.
\end{exercice}

Ce résultat est à connaître.

\pagebreak

\section{Loi exponentielle}

\subsection{Définition et espérance}

\begin{definition}
  On appelle \emph{loi exponentielle} de paramètre $\lambda>0$ une loi
  de probabilité dont la densité de probabilité est donnée par
  $f:x\mapsto \lambda e^{-\lambda x}$
\end{definition}

Il faut néamoins vérifier que cette fonction définit bien une densité de
probabilité.

\begin{exercice}
  Soit $\lambda$ un réél strictement positif.
  \begin{enumerate}
    \item Calculer l'intégrale $\int_0^x \lambda e^{-\lambda t} \diff t$
      en fonction de $x$ et de $\lambda$.
    \item Donner la limite de la fonction $F:t \mapsto 1 - e^{-\lambda
      t}$ en $+\infty$.
    \item Conclure.
  \end{enumerate}
\end{exercice}

Le calcul d'une probabilité lorsqu'une variable aléatoire $T$ suit une
loi exponentielle s'obtient généralement par le calcul direct.

\begin{exercice}
  Soient $a$ et $b$ deux réels tels que $0\leqslant a \leqslant b$.
  Calculer :
  \begin{itemize}
    \item $\p(T \leqslant a) = \int_0^a \lambda e^{-\lambda x} \diff x$ ;
    \item $\p(T \geqslant a) = \p(T > a )$
    \item $\p(a \leqslant T \leqslant b)$
  \end{itemize}
\end{exercice}

On peut s'intéresser à l'espérance d'une telle variable aléatoire.

\begin{propriete}
  L'espérance d'une variable aléatoire $T$ qui suit une loi
  exponentielle de paramètre $\lambda$ est $E(T) = \frac1{\lambda}$.
\end{propriete}

\begin{exercice}
  Soit $\lambda$ un réél strictement positif.
  \begin{enumerate}
    \item Calculer l'intégrale $\int_0^x \lambda te^{-\lambda t} \diff
      t$ en fonction de $x$ et de $\lambda$ (on pourra utiliser une
        intégration par parties ou poser $f:t\mapsto \lambda
        te^{-\lambda t}$ et étudier $\lambda f + f'$.)
    \item Donner la limite de la primitive trouvée ci-dessus en
      $+\infty$.
    \item Conclure.
  \end{enumerate}
\end{exercice}

\subsection{Loi de vieillissement « sans mémoire»}

Dans cette partie, $t$ et $h$ sont deux nombres réels positifs et on
note $\p(A | B) = \p_B(A)$ la probabilité de l'événement $A$ sachant que
l'événement $B$ est réalisé.\\
On rappelle également que $\p(A | B) = \frac{\p(A\cap B)}{\p(B)}$.

\begin{definition}
  On dit qu'un processus aléatoire est «sans mémoire» lorsque la
  probabilité qu'il tombe en panne au bout de $t+h$ sachant qu'il a
  fonction $t$ heures est identique à la probabilité qu'il cesse de
  fonctionner au bout de $h$ heures. Mathématiquement, si $T$ est la
  variable aléatoire qui à un composant associe sa durée de vie en
  heures, alors $\p(T > t+h | T > t) = \p(T > h)$
\end{definition}

Une autre formulation est de dire que la probabilité $\p(T > t+h | T >
t)$ ne dépend pas de $t$.

Ce modèle est particulièrement vrai dans le cadre de la modélisation des
composants électroniques non chimiques qui entre pour une partie non
négligeable des produits fabriqués aujourd'hui. Ce modèle est également
utilisé pour décrire un taux de panne constant sur des systèmes
complexes.

Ce modèle n'est en revanche pas valide pour les pièces présentant une
usure mécanique qui est généralement quantifiable et donc ne fait pas
appel à des méthodes probabbilistes.

\begin{propriete}
  La loi exponnentielle est une loi «sans vieillissement».
\end{propriete}
\begin{preuve}
  \begin{exercice}
    Soit $T$ une variable aléatoire qui suit une loi exponnentielle de
    paramètre $\lambda$.
    \begin{enumerate}
      \item Exprimer $\p(T > t+h | T > t)$ en fonction de $\p(T > t+h)$
        et de $\p(T > t)$.
      \item Exprimer $\p(T > t+h)$ et $\p(T > t)$ avec des
        exponnentielles.
      \item Interpréter $e^{-\lambda h}$ comme une probabilité.
      \item Conclure.
    \end{enumerate}
  \end{exercice}
\end{preuve}

On admet que la réciproque est également vraie et que les variables
aléatoires qui suivent des lois exponnentielles sont les seules
variables aléatoires «sans vieillissement». En fait, cette démonstratio
n'est possible que si on dispose d'une théorie des équations
différentielles qui permet de réellement montrer l'existence et
l'unicité de la fonction exponnentielle et qui a permis de construire la
loi exponnentielle à partir des lois «sans usure».

\pagebreak

\section{Loi normale}

Dans cette partie, on parle indiférement de la loi de Laplace-Gauss ou
de la loi normale pour désigner la même densité de probabilité.

\subsection{Le cas «simple» : la loi normale centrée-réduite}

L'appellation de centrée-réduite sera justifiée par la suite.

\begin{definition}
  La fonction $\varphi : x\mapsto \frac1{\sqrt{2\pi}}e^{-\frac{x^2}2}$
  s'appelle \emph{densité de probabilité de la loi normale}.
\end{definition}

On dit aussi loi de Laplace-Gauss du nom des premiers mathématiciens à
l'avoir étudié.

\begin{propriete}[admise]
  \[ \int_{-\infty}^{+\infty} e^{-\frac{x^2}2} = \sqrt{2\pi} \]
\end{propriete}

Ce résultat général est dur à obtenir, mais il nous assure que la
fonction $\varphi$ définie ci-dessus est bien une densité de
probabilité.

\begin{exercice}
  Justifier cette affirmation.
\end{exercice}

\begin{definition}
  On dit qu'une variable aléatoire $Z$ suit la loi normale
  centrée-réduite lorsque $\p(a\leqslant Z \leqslant b) = \int_a^b
  \varphi(x)\diff x$. On note $Z \leadsto \mathcal{N}(0,1)$.
\end{definition}

\begin{remarque} La notation $\mathcal{N}(0,1)$ sera expliquée
  rapidement par la suite. Ces deux valeurs 0 et 1 sont à connaître pour
  la loi centrée-réduite : elles permettent le calcul effectif d'une
  probabilité avec la calculatrice.
\end{remarque}

Il est aussi parfois commode d'envisager la probabilité pour $Z$ d'être
entre $a$ et $b$ comme l'aire sous une courbe. Cette méthode avait un
grand avantage de simplification avant l'arrivée des calculatrices. En
effet, la primitive $\Phi$ de $\varphi$ ne pouvant s'exprimer avec des
fonctions usuelles, les valeurs de $\Phi$ étaient tabulées sur la
demi-droite des $x$ positifs.

\begin{propriete}
  $\p(a \leqslant X \leqslant b) = \Phi(b) - \Phi(a)$
\end{propriete}
\begin{preuve}
  $\Phi$ est, par définition, une primitive de $\varphi$.
\end{preuve}

Les $x$ positifs suffisent, en vertu de cette propriété :

\begin{propriete}
  Pour tout $x$ réel, $\Phi(-x) = 1 - \Phi(x)$
\end{propriete}
\begin{preuve}
  Par symétrie de $\varphi$ et en utilisant le fait que le poids total
  est de 1.
\end{preuve}

Cependant, ce résultat peut servir lors de certaines démonstrations.

Calculons l'espérance d'une variable aléatoire suivant une telle loi.

\begin{exercice}
  Soit $a$ un nombre réel.
  \begin{enumerate}
    \item Calculer $\int_0^a x\varphi(x) \diff x$.
    \item En déduire $\lim_{a\to+\infty} \int_0^a x\varphi(x) \diff x$.
    \item Conclure sur l'espérance $E(Z)$.
  \end{enumerate}
\end{exercice}

\begin{definition}[Variance]
  Soit $Z$ une variable aléatoire qui suit une loi normale
  centrée-réduite. On appelle \emph{variance} et on note $\sigma^2$ le
  nombre défini par $\sigma^2 = \int_{-\infty}^{+\infty} x^2\varphi(x)
  \diff x$.
\end{definition}

\begin{propriete}
  La variance $\sigma^2$ d'une variable qui suit la loi normale
  centrée-réduite est $\sigma^2 = 1$
\end{propriete}
\begin{preuve}
  Exercices 82 et 83 p 432
\end{preuve}

\begin{propriete}[admise]
  $\sigma^2 = E((X - E(X))^2)$
\end{propriete}

\begin{exercice}
  Montrer que cette dernière proposition conduit à la définition
  proposée de la variance. Interpréter la variance en terme de
  moyenne.
\end{exercice}

Dans la littérature, on note souvent la moyenne $\mu$ et la variance
$\sigma^2$.

\pagebreak

\subsection{Le cas général}

On a vu, dans la partie précédente, un cas particulier de loi normale
lorsque les paramètres $\mu$ et $\sigma^2$ valent respectivement 0 et 1,
ce qui n'est pas toujours le cas.

On peut aisément généraliser et considérer une variable aléatoire $X$
qui suit une loi normale d'espérance $\mu$ quelconque et $\sigma^2 > 0$.

\begin{propriete}
  Si $X \leadsto \mathcal{N}(\mu,\sigma^2)$, alors $Z = \frac{ X -
  \mu}{\sigma^2}$ suit une loi normale centrée-réduite.
\end{propriete}

\begin{exercice}
  On considère une variable aléatoire $X$ qui suit une loi normale de
  paramètres $\mu$ et $\sigma^2$.
  \begin{enumerate}
    \item Donner l'expression de $X$ en fonction de $Z$ lorsque $Z$ suit
      la loi normale centrée-réduite.
    \item En déduire l'expression de la densité de probabilité de $X$.
  \end{enumerate}
\end{exercice}

En règle général, le calcul des valeurs de la loi normale se fait
désormais à la calculatrice.

\section{Applications et autres exemples}

\subsection{Quelques valeurs particulières à connaître}

Avant de se lancer dans le calcul à proprement parler, démontrons une
première propriété.

\begin{propriete}
  Soit $\alpha \in \interoo{0;1}$ et $X$ une variable aléatoire qui suit
  la loi normale centrée-réduite. Il existe un unique réel positif
  $u_{\alpha}$ tel que $\p(-u_{\alpha} \leqslant X \leqslant) = 1 -
  \alpha$.
\end{propriete}

\begin{exercice}
  On se donne $\alpha \in \interoo{0;1}$.
  \begin{enumerate}
    \item Exprimer $\Phi(x)$ en fonction de $\alpha$ où $\Phi$ est la
      primitive (aire sous la courbe de la densité de densité de
      probabilité) qui vaut $\frac12$ en 0.
    \item Vérifier que $\Phi$ respecte les hypothèses d'un théorème
      important du cours de Terminale.
    \item Conclure.
  \end{enumerate}
\end{exercice}

En particulier, il faut connaître : $u_{\np{0.05}} = \np{1.96}$ et
$u_{\np{0.01}} = \np{2.58}$. De façon équivalente, on obtient $\p(-1,96
\leqslant X \leqslant 1,96) \approx 0,95$ et $\p(-2,58 \leqslant X
\leqslant 2,58) \approx 0,99$.

Il est intéressant de noter que ce sont des cas particuliers de points
plus généraux qu'on peut exprimer ainsi :
\begin{itemize}
  \item $\p(\mu - \sigma \leqslant X \leqslant \mu + \sigma) \approx
    0,63$
  \item $\p(\mu - 2\sigma \leqslant X \leqslant \mu + 2\sigma) \approx
    0,95$
  \item $\p(\mu - 3\sigma \leqslant X \leqslant \mu + 3\sigma) \approx
    0,99$
\end{itemize}

\pagebreak

\subsection{Approximation d'une loi binomiale par une loi normale}

On a vu dans une activité que la loi binomiale pouvait, sous certaines
hypothèses, permettre de modéliser un nouveau type de loi, la loi de
Poisson. On conservait cependant un invariant : l'espérance des deux
lois restait la même.

On peut de même approcher, sous certaines conditions la loi binomiale
par une loi normale, c'est le théorème de Moivre-Laplace.

\begin{theoreme}[Admis]
  Soit $X$ une variable aléatoire qui suit une loi binomiale
  $\mathcal{B}(n,p)$ et $Z = \frac{X - E(X)}{\sigma(X)} = \frac{X -
  np}{\sqrt{np(1-p)}}$.

  Alors $\lim_{n\to+\infty} P(a \leqslant Z \leqslant b) = \int_a^b
  \varphi(x) \diff x$.
\end{theoreme}

\begin{remarque}
  Pour $n \leqslant 30$ ou $np$ ou $n(1-p) \leqslant 5$, cette
  approximation n'est pas valide.
\end{remarque}



\end{document}
